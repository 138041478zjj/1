# 高级统计学习

## 第一章 导论

勒让德、高斯：线性回归

费舍尔：线性判断分析、$p$值

内尔德、韦德伯恩：广义线性回归

布赖曼、弗里德曼、奥申、斯通：分类回归树

哈斯贴、提布施瓦尼：广义可加模型



## 第二章 统计学习

1.不同方法在精度和可解释性上的均衡（共五类）：P18

2.估计*f*的原因（三点，一条预测，两条推断）：P13-14

3.非参数方法和参数方法相比的优缺点（各一点）：P16

**4.模型光滑度变高的原因：①预测变量的增多，例如引入交互项、高次多项式等等；②模型和方法的选择**

**5.维数灾难：在高维中空间，邻域不再是局部的，这将导致给定的观测附近没有邻点，因此不能通过局部平均来进行预测。**

**6.有指导学习：对于每一个预测变量的观测值$x_i$，都有相应的响应变量的观测$y_i$.**

**7.无指导学习：只有预测变量的观测值$x_i$，这些向量没有相应的响应变量的观测$y_i$.**

**8.可约误差和不可约误差：①大体上，当所选的$\hat{f}$不是*f*的一个最佳估计时，对模型估计的不准确也会引起一些误差，但是这个误差是可约的，只要选择更合适的统计学习方法去估计*f*就可能降低这种误差；②*Y*同时也是关于$\epsilon$的函数，按照定义，$\epsilon$是不能用*X*去预测的。因此，与$\epsilon$有关的变量也影响到预测的准确性，这部分误差被称为不可约误差，因为我们无论对*f*的估计多么准确，我们都不能减少$\epsilon$引起的误差。**



## 第三章 线性回归

1.*MSE、RSS、TSS*以及残差$e_i$的公式

2.标准误差可以用于计算置信区间：68%置信区间、95%置信区间、99%置信区间

3.*t*统计量的公式，以及*t~t（n-2）*

4.*RSE、*$R^2$的公式

**5.为什么使用*F*统计量而不是多重假设检验？答：为了避免多重假设检验使得假阳性显著增加的问题。**

**6.标准线性回归模型的假设条件：可加性假设和线性假设。①可加性假设是指：预测变量$x_j$的变化对响应变量*Y*产生的影响与其他预测变量的取值无关；②线性假设是指：无论$x_j$取何值，$x_j$变化一个单位引起的响应变量*Y*的变化是恒定的。**

**7.实验分层原则：如果模型中含有交互项，那么即使主效应的系数的*p*值不显著，也应包含在模型中。**

8.可能存在的问题：数据的非线性、误差项自相关、误差项方差非恒定、离群点、高杠杆点、共线性

**9.共线性的解释：由于共线性降低了回归系数估计的准确性，它会导致$\hat{\beta_j}$的标准误变大，从而导致*t*统计量的下降。因此，如果存在非线性，我们可能无法拒绝$H_0:\beta_j=0$这意味着：假设检验的效力（即：正确地检验出非零系数的概率）被共线性减小了。**

10.针对线性模型，利用$RSS$来进行系数估计，利用$R^2、RSE$来评估模型。

11.置信区间：以$\hat{\beta_1}$的95%置信区间为例，下述区间：$$[\hat{\beta_1}-2*SE(\hat{\beta_1}),\hat{\beta_1}+2*SE(\hat{\beta_1})]$$有95%的可能性包含$\hat{\beta_1}$的真实值。

**12.多元线性回归模型中，系数$\hat{\beta_j}$的解释为：在其他预测变量保持不变的情况下，$X_j$增加一个单位对$Y$产生的平均效果。**

13.预测变量和响应变量之间存在的关系是**线性关系**，而非因果关系。

14.哑变量个数总是比水平数少1.

**15.线性回归（参数法）和*KNN*方法（非参数法）的比较：①优点：线性回归需要估计的参数少，容易拟合，系数有简单的解释，容易进行统计显著性检验；②缺点：线性回归的假设过强：如果指定的函数形式与实际相差太远，则表现不佳。③选择：如果选定的参数形式接近*f*的真实形式，则参数法更优；若每个预测变量仅有少量观测，参数法往往由于非参数法；当真实关系为线性时，*KNN*略逊于线性回归，但在非线性的情况下，*KNN*大大优于线性回归；在更高维的情况下，*KNN*的表现往往不如线性回归（维数灾难）。**

16.离群点、高杠杆点、共线性的概念以及共线性的危害：P66-69



## 第四章 分类

1.为什么在分类问题上，线性回归不可用？（两点）：P90-91

**2.为什么进行判别分析而不是逻辑斯谛回归的原因：①当类别的区分度高的时候，逻辑斯谛回归模型的参数估计不够稳定，这点在线性判别分析中是不存在的；②如果样本量*n*较小，而且在每一类响应分类中预测变量*X*近似服从正态分布，那么线性判别分类模型比逻辑斯谛回归模型更加稳定；③响应分类多余两类时，线性判别分析应用更加普遍。**

3.线性判别分析的决策边界的证明：P97

**4.混淆现象：P96（与线性回归一样，只用一个预测变量得到的结果可能与多个预测变量得到的结果完全不一样，尤其是这些因素之间存在相关性时更是如此）**

5.参数方法（逻辑斯谛回归、LDA、QDA）和非参数方法（KNN）之间的比较：P105

6.对于逻辑斯谛回归，无法利用$RSS$来对模型进行选择（由于逻辑斯谛回归模型输出的是概率而非预测值，因此无法使用*RSS*）；逻辑斯谛回归用最大似然估计来进行系数的选择。

**7.对于线性回归，采用最小二乘法确定系数；对于逻辑斯谛回归，采用极大似然估计确定系数；对于线性判别分析，通过估计的正态分布均值和方差计算出来的。**



## 第五章 重抽样方法

**1.验证集方法的缺点：①测试错误率的验证法估计的波动很大，这取决于具体哪些观测被包括在训练集中，哪些观测被包含在验证集中；②在验证法中，只有一部分观测被用于拟合模型。由于被训练的观测很少，统计方法的表现越不好，这意味着，验证集错误率很有可能高估在整个数据集上拟合模型得到的测试错误率。**

**2.验证集方法的优点：①提供了测试误差的直接估计，并且对真实的潜在模型有较少的假设；②使用范围广，即使是在难以精确确定模型自由度或者难以估计误差方差$\sigma^2$的情况下。**

**3.LOOCV和K折交叉验证方法的比较：①当*K=N*时，即为留一交叉验证法。此时，得到的模型具有低偏差、高方差的特点；②K折交叉验证相比于LOOCV方法，具有计算上的优势，计算量小；③采用K折交叉验证的方法，由于训练集只占整个数据集的$\tfrac{K-1}{K}$，因此训练错误率可能会高估测试错误率。**

**4.自助法和K折交叉验证方法的比较：①自助法允许我们使用计算机来模拟获得新数据集的过程，可以在不产生额外样本的情况下评估我们估计的变化性；②自助法存在某些训练数据被多次抽取并应用到训练中去，导致其可能严重低估测试误差。**

5.自助法得到的样本约占总数据集的$\tfrac{2}{3}$证明方法：P131

6.如果数据是一个时间序列，我们不能采用自助法对数据进行简单地有放回抽样。



## 第六章 线性模型选择和正则化

**1.对于最小二乘法：利用*RSS*来进行系数估计和模型选择；对于逻辑斯谛回归模型，利用偏差代替*RSS*来进行系数估计和模型选择。**

**2.对于所有的子集选择方法：先根据*RSS、$R^2$*来确定各个模型；再根据交叉验证预测误差、$C_p、AIC、BIC、Adjust-R^2$来确定最终的模型。**

3.算法6.1 最优子集选择：P141

**4.最优子集选择的缺点：①最优子集选择方法简单直观，但计算效率不高；当*p>40*时，该方法不具计算可行性；②随着搜索空间的增大，通过此方法得到的模型对数据不具备很好的预测能力；③从一个巨大的搜索空间得到的模型通常会有过拟合和系数估计方差高的问题。**

5.算法6.2 向前逐步选择：P143

6.算法6.3 向后逐步选择：P144

**7.向后逐步选择需要满足$n>p$的条件；相反，向前逐步选择即使在$n<p$的情况下也可以使用。因此，当*p*非常大的时候，向前逐步选择是唯一可行的办法。**

8.$C_p、AIC、BIC、Adjust-R^2$公式：P145-146

**9.岭回归为什么能够提升最小二乘法的效果？答：与最小二乘法相比，岭回归的优势在于它综合权衡了方差和误差；随着$\lambda$的增大，拟合结果的光滑度在降低，虽然方差降低，但是偏差增大。其中：$\lambda=0$对应于最小二乘法估计，此时方差很高，但是没有偏差。随着$\lambda$的增大，以偏差略微增大的代价，岭回归系数的缩减使得预测值方差显著减少。**

**10.岭回归和Lasso法的比较：①在性质上相似：随着$\lambda$的增大，方差减小而偏差增大；②当最小二乘法的估计出现较大方差时，Lasso法可以得到更精确的预测结果；③当一小部分的预测变量是真实有效的，而其他的预测变量系数非常小或者等于零时，Lasso法更好；④当响应变量是很多预测变量的函数并且这些变量的系数大致相等时，岭回归较好；⑤对于真实的数据集，与响应变量相关的变量个数是无法先验知道的，可以通过交叉验证等技术来确定哪种方法对特定数据集更好；⑥通过岭回归得到的最终模型包含全部的*p*个变量；当变量个数*p*非常大时，不便于模型解释。对于Lasso法来说：根据$\lambda$的不同取值，可以得到包含不同变量个数的模型。**

**11.在构造主成分之前，应当对数据进行标准化的理由：标准化保证了所有变量在相同的尺度上。如果不做标准化，方差较大的变量将在主成分中占据主导地位，变量的尺度将最终影响主成分回归模型。**

**12.高维数据分析结果的解释：P166**

**13.岭回归假设密度函数$g(x)$服从高斯分布，Lasso法假设密度函数$g(x)$服从Laplace分布。**

**14.在解释高维数据拟合模型的误差和拟合效果的时候，绝不能在训练数据集上用误差平方和、*p*值、$R^2$统计量或者其他传统的对模型拟合效果的度量方法来证明高维情况下模型拟合的效果。例如，当$p>n$时，可以很容易地得到$R^2=1$的模型，然而事实并非如此。重要的是在独立的测试集进行验证或者进行交叉验证。**



## 第七章 非线性模型

1.非线性模型：多项式回归、阶梯函数、回归样条（三个约束条件，自由度为*K+4*）、自然样条（四个约束条件，自由度为*K*）、光滑样条、局部回归、广义可加模型。

**2.光滑样条采用二阶导数的平方作为惩罚项的原因：一阶导数衡量的是函数在*t*处的斜率，而二阶导数衡量的是函数斜率的变化程度，即为函数的粗糙度。如果*g（t）*在*t*处波动很剧烈，则其绝对值大，反之则接近于0；在式中，惩罚项会使得*g*变光滑。因此，$\lambda$的值越大，函数*g*就越光滑。**

3.算法7.1 局部回归模型：P195（**两个重要参数：①权重*K* ②间距*s***）

**4.解释局部回归模型参数*s*的作用：间距在这里的作用相当于调节参数$\lambda$在光滑样条中的作用，它控制着非线性拟合的光滑性。实际上，选择一个较小的*s*相当于使用局部观测建模，拟合的效果会剧烈起伏；相反，当*s*很大时，相当于使用所有的观测数据做一个全局的拟合，因此得到的模型更加光滑。**

**5.GAM模型的缺点：GAM模型的形式被限定为可加形式，在多变量的情况下，会忽略有意义的交互项。**

**6.GAM模型的优点：①GAM模型可允许对每一个$X_j$都拟合一个非线性$f_i$，可自动地对被标准的线性回归模型所忽略的非线性关系进行建模，不需要手动地为每一个变量设置不同的变形形式；②非线性拟合模型能将响应变量预测得更精准；③模型是可加的，能在保持其他变量不变的情况下查看每个变量$X_j$对$Y$单独的影响效果；④针对变量$X_j$的函数$f_i$的光滑性也能通过对自由度的分析得到。**

**7.（P186 分析）为什么图像的右半部分的置信区间稍宽？答：尽管建模数据总的样本是足够的，但是只有79个高收入的人，这导致了估计系数有较大的方差，因而置信区间也较宽。**



## 第八章 基于树的方法

1.对于树的方法，*RSS*计算方法

2.代价复杂性剪枝（最弱联系剪枝）的最优化目标

3.建立回归树的算法：P213

4.算法8.1 建立回归树（基于代价复杂性剪枝）：P215

5.分类树的错误率、基尼指数以及互墒的公式：P216-217

**6.基尼指数*G*：衡量结点纯度的指标。如果它的值较小，就意味着某个结点包含的观测值都几乎来自于同一类别。**

**7.决策树的优缺点：①决策树解释性强，在解释性方面甚至比线性回归更方便；②决策树更加接近人的决策模式；③树可以用图形表示，非专业人士也可以轻松解释；④树可以直接处理定性的预测变量而不需创建哑变量；⑤树的预测准确性一般无法达到其他回归和分类方法的水平。**

8.装袋法算法：P220（对于回归问题：取平均值；对于分类问题，利用多数投票）

**9.算法8.2 对回归树应用提升法：P223**

**10.决策树、装袋法、随机森林和提升法：①决策树是用于回归和分类的简便且易于解释的模型；②然而，在预测精度方面，树一般无法达到其他回归和分类方法的水平；③装袋法、随机森林和提升法是提高树预测精度的有效方法；④随机森林和提升法，是监督学习方法，然而他们的结果可能很难解释。**



## 第九章 支持向量机

**1.软间隔中，参数*C*的作用：参数*C*代表了我们能够容忍的穿过间隔（以及超平面）的观测的数目和严重程度。如果*C>0*，那么只有不超过*C*个观测可以落在超平面错误的一侧。**

**2.支持向量机和逻辑斯谛回归的比较：①当类别几乎可分离时，SVM比LR做的更好，LDA也是如此；②不可分离的时候，LR比较适合；③如果想得到各个类别的概率值，选择LR；④对于非线性边界，核支持向量机较好；也可以使用核函数连同LR和LDA，但计算开销较大。**

**3.支持向量：落在间隔和间隔错误一侧的观测叫做支持向量。**



## 第十章 无指导学习

**1.载荷向量：载荷向量定义了一个在向量空间上数据变异最大的方向。**

**2.为什么需要对载荷进行标准化？答：为了防止载荷的绝对值任意大而导致方差无限大，限制这些载荷的平方和为1.**

3.第一主成分载荷向量是*p*维空间中一条最接近*n*个观测的线。

**4.变量的标准化：主成分受变量度量单位的选择影响，会导致结果的任意性。**

**5.数据最多有$min(n-1,p)$个主成分。**

**6.PCA和聚类分析比较：①PCA试图寻找观测的一个低维来解释大部分方差；②聚类分析试图从观测中寻找同质子类。**

**7.*K*均值分类和系统聚类的比较：*K*均值聚类试图将观测划分到事先规定数量的类中，而系统聚类并不需要事先规定所需的类数。**

8.*K*均值聚类的最优化目标：P268

9.算法10.1 *K*均值聚类：P269

10.恒等式：P269

11.算法10.2 系统聚类法：P274

12.四种距离形式以及倒置（inversion）现象：①四种距离形式：最长距离法（Complete）、最短距离法（Single）、类平均法（Average）以及重心法（Centroid）；②倒置现象：使用重心法时，在谱系图中两个类的合并点可以再这两个类中任意一个类的下方。

13.系统聚类法中的问题：①用什么指标衡量相异度？（欧氏距离、相关性距离）②选择怎样的距离计算距离？（最长距离法（Complete）、最短距离法（Single）、类平均法（Average）以及重心法（Centroid））③在谱系图中的哪个位置切割出不同的类？④在*K*均值聚类中，数据分成多少类比较合适？·（没有准确答案）



## R语言中函数

线性拟合:lm()

逻辑斯谛回归:glm(),设置family=“binomial”

线性判别分析:lda()，没有family选项

二次判别分析:qda()，没有family选项

KNN:knn()，四个参数分别为：训练集、测试集、包含训练观测类标签的向量、$K$的值

交叉验证（详细用法）

最优子集、向前逐步选择、向后逐步选择:regsubsets(),设置参数method=“forward”或者“backward”

**岭回归、Lasso法:glmnet(),设置alpha=0为岭回归，alpha=1为Lasso法**

主成分回归:pcr()

偏最小二乘模型:plsr()

多项式模型:lm()以及poly()函数的结合

阶梯函数（注意用法）:在函数内部使用cut()

（三次）回归样条:bs()

自然样条:ns()

光滑样条（两种方法）:①s();②smooth.splines()

局部回归:loess()

广义可加模型（详细用法）:gam()

树（使用交叉验证和未使用交叉验证）:tree()

剪枝树（针对分类树和回归树）：①分类树：prune.misclass();②回归树：prune.tree()

装袋法、随机森林:randomForest()

**提升法:gbm()**

支持向量机:svm()，参数kelnel=”linear“为线性核函数；kelnel=”polynomial“为多项式核函数，同时需要设置参数degree；kelnel=”radical“为径向核函数，同时需要设置参数gamma

默认10折交叉验证的函数：tune（）函数，被包含在e1071库中

**主成分分析:prcomp()，scale=TRUE代表进行标准化**

$K-means$聚类:kmeans()

**系统聚类:hclust()**
